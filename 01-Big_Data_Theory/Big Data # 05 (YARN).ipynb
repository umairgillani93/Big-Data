{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN (Yet Another Resource Negotiator):\n",
    "Whether we are dealing with __Spark__ or other things, __YARN__ is equally important to learn and understand.\n",
    "\n",
    "It's always better to do the processing of each data block on each of the datanodes, instead of transporting the data. Therefore, we need a system to perform the distributed computing. i.e we need a system that runs our programs in a __Distributed Way__, this is where __YARN__ comes into play!\n",
    "\n",
    "__YARN__ is a kind of operating system, which is not used by us directly but it is used by the tools such as __Spark__ or __MapReduce__.\n",
    "\n",
    "So, we interact with the __Spark__ or __MapReduce__ who intereact with __YARN__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Processing Problem:\n",
    "\n",
    "Question: Suppose we have 1 million pictures how much time would it take for a single computer to resize a million profile pics? If it takes 20 ms to read 1 picture, and 5 ms to resize it?\n",
    "\n",
    "Ans: 10^6 * 20 = 20 ms (we won't consider the resizing time, the whole process is done in parellel). Also if we increase the cores of __Computer__ yet it won't imporve the computations. The only thing that improves the __computation__ would be __Distributed Computing__. i.e: If there is 1000 nodes, each having 1000 pics (1000 * 1000 = 1 million)\n",
    "\n",
    "10^6 * 10^-3 * 10^-3 * 20 / 3600 = time for uploading and resizing 1M pics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Node time in hrs: 5.556\n",
      "Thousand Nodes time in hrs: 0.006 \n"
     ]
    }
   ],
   "source": [
    "time_in_hrs = (10**6 * 20* 10**-3) / 3600\n",
    "print('Single Node time in hrs: {0:0.3f}' .format(time_in_hrs))\n",
    "\n",
    "# for 1000 nodes\n",
    "\n",
    "thousand_nodes_time = time_in_hrs / 1000\n",
    "print('Thousand Nodes time in hrs: {0:0.3f} ' .format(thousand_nodes_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop MapReduce Engine (Hadoop 1.0) VS HDFS Cluster (Hadoop 2.0):\n",
    "\n",
    "In __MapReduce Engine__ Hadoop 1.0, they created something called a __Job Tracker__ and on each machine where __Datanode__ was there __Task Tracker__. Task Tracker was installed on every machine where Datanode was installed. It's not mendatory but important to keep the _Task Tracker_ on machines where _Datanode_ was installed. \n",
    "\n",
    "So, in MapReduce Engine the end-user would submit their job to the Job Tracker, and Job Tracker would internally distribute the work the various Task Trackers. e.g example to get the images resized. The __Job Tracker__ also points out the corresponding __Datanode__ from which the specific __Task Tracker__ has to pull images from. Every __Task Tracker__ pulls data from it's own __Datanode__.\n",
    "\n",
    "So, basically there are two things we did here:\n",
    "\n",
    "1. Utilized the CPUs of so many machines\n",
    "2. Utilized their disk-read speed in parallel.\n",
    "\n",
    "The above workflow would be done by something called MapReduce. We would write two kind of functions __Mapper__ and __Reducer__ and each Task Tracker will first execute the Mappers followed by the Reducers.\n",
    "\n",
    "The downside of this approach was, we could only execute Map and Reduce operations and couldn't get anything else done. MapReduce was the only Engine responsible for not only performing operations but also keeping track of How much memory has left and hardware parameters, sorting of data and everything! \n",
    "\n",
    "__MapReduce:__ (Cluster, Resource management & Data processing) -> __HADOOP 1.0__\n",
    "\n",
    "In HDFS Cluster, more precisely __Hadoop 2.0__ MapReduce functionality is broken into __MapReduce 2.0__ and another component called __YARN__. Here, the MapReduce does only the __Data Processing__ work and __YARN__ is responsible of __Clustering__ and __Resource Management__ operations. Also, __YARN__ could be used by not only MapReduce kind of systems, but any kind of systems.\n",
    "\n",
    "On the top of YARN, other then MapReduce, we have __Tez__, __HBase__, __Storm__, __Giraph__, __Spark__, __OpenMPI__ etc.\n",
    "\n",
    "__MapReduce:__ (Data processing)\n",
    "__YARN:__ (Cluster and Resource management)\n",
    "__Others:__ (Data processing) -> __HADOOP 2.0__\n",
    "\n",
    "Today the __Spark__ is actually more powerful that __MapReduce__ because of the splitting we explained above!\n",
    "\n",
    "Here are the respective job roles for each MapReduce1 and YARN:\n",
    "\n",
    "**MapReduce1** -> Job Tracker, Task Tracker and Slot\n",
    "\n",
    "**YARN** -> (Resource Manager, Appication Master, Timeline Server), Node Manager, Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does YARN Work?\n",
    "\n",
    "In __YARN__ we have a Resource Manager (Central Agent) which gives instructions to multiple __Node Managers__ kind of acts like a Job Tracker and Task Trackers in HADOOP 1.0. The Client will talk to the __Resource Manager__ and asks it to get the work done, in such away that Resource Manager will ask one of the Node managers to start the first process, this first process is called __App Master__. And the rest of the work of the Client will be done by this App Master, as it takes the position of Client afterwards. The App Master will continue asking to the Resouce Manager, it order to fulfill the task.\n",
    "\n",
    "__Node Manager__ is installed one per Machine, which means all the __Data Nodes__ that we have, we will generally install __Node Manager__ on each of the Data Nodes\n",
    "\n",
    "__Container__ is a process, which runs the user's work andenforces the security on the Client processes. Like,\n",
    "* Memory Consumption\n",
    "* Resource Access and so on..\n",
    "\n",
    "## Workflow:\n",
    "* Client submits the work to the Resource Manager\n",
    "* Resource Manager will launch the Application Master in one of the Containers. Resource Manager will get the Container from Node Manager to lauch the Application Master inside it.\n",
    "* Here on, the Application Master will behave like a Client and will register itself with Resource Manager. Application Master will ask the Resource Manager give me more Containers to get the work done, and the actual work is done by the Container. e.g the image resizing example we talked about.\n",
    "* RM will allocate the Containers by consulting the Node Managers\n",
    "* Node Manager lauches the Apps in these Containers.\n",
    "* Node Manager talks back to RM about the resource usage.\n",
    "* Clients talk to Application Master for the work status.\n",
    "* On completion, AM deregisters with RM & shuts down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
