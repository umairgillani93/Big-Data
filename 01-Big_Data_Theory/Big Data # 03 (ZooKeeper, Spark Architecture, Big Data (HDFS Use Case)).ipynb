{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZooKeeper:\n",
    "\n",
    "A __Distributed Coordination__ Service for __Distributed Applications__. In very simple words, it is a __Central Store__ of __key,values__ using which Distributed Systems can coordinate. It's a service that handles heavy number of requests received. ZooKeeper is also used as load balancing across multiple machines.\n",
    "\n",
    "Since, it needs to be able to handle load, ZooKeeper itself runs on many machines. ZooKeeper provides simple set of primitives, and very easy to program too. It uses data model like __directory tree__.\n",
    "\n",
    "It is used for the following things:\n",
    "* Synchronization\n",
    "* Locking\n",
    "* Maintaining configurations\n",
    "* Failover management\n",
    "\n",
    "And it doesn't suffer from __race conditions__ and __deadlocks__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Architecture:\n",
    "\n",
    "Spark provides a way to process the data. Spark core can run on any of the following resource managers:\n",
    "\n",
    "* Hadoop YARN\n",
    "* Amazon EC2\n",
    "* Standalone\n",
    "* Apache Mesos\n",
    "\n",
    "Spark can also read from many data resources like, HDFS, HBase, Hive, Cassandra and Tachyon etc. For basic computing we can use __Spark Core__, but if our data is too large, we structurize it and use dataframes to querry the tables using __SQL__, and also can use __GraphX__, __MLlib__ for respective purposes.\n",
    "\n",
    "### Side Notes:\n",
    "\n",
    "Once the data is extracted, transformed and loaded into Data frames or SQL, it can then be accessed by __BI__ tool such as __Tableau__ to querry the data frames and show them in reports. Similarly if we are in __Hadoop__ echosystem, we can querry the data frames and connect with __Hive__ using our __BI__ tool in order to represent it to the front-end user.\n",
    "\n",
    "For __BI__ to work efficienty, we have to translate unstructured data to structured, clean it up and keep it ready for the reports to be published\n",
    "\n",
    "When we store data on __HDFS__, it breaks it down into parts and keeps it on many Machines, and then we do all kinds of processing on top of it, and combine data using __Spark__, __MapReduce__ or __Hive__ and then save the results back to __HDFS__. Once this this is done, we expose the data via __Hive__ server so that our traditional can querry that data using our existing __Visualizing Tools__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data (HDFS):\n",
    "\n",
    "Consider a case study, in which an __Ecommerce__ website is trying to create a __Recommender System (collaborative filtering)__. Following are the steps it needs to go through:\n",
    "\n",
    "* Where is the data? -> Log files (Text files that Ecommerce Servers (Multiple servers, server1, server2,..., serverN) save based on user activity)\n",
    "* What is the end objective? -> _you may also like these (recommendations)_\n",
    "* Take this data from multiple servers and import it to HDFS using __Flume__\n",
    "* Use __Spark__ to process the data in HDFS. The __Spark Script__ (Log Parser) would run on top of __YARN__ and put the data to __Hive (SQL Interface)__\n",
    "* After the data (querryable & tabular) from HDFS goes to __Hive Interface__. Analysts can use __SQL__ to write querries and extract the data from __Hive__ (converts SQL querries to MapReduce)\n",
    "* The Analyst can also visualize the data using BI tool such as __Tableau__ or __BIRT__.\n",
    "* From here, we can further write our __MLlib__ program (Spark Script) which can read data from __Hive__ and generate the __Recommendations__ and push it to the __Web Server Database (MySQL or anything)__. In case, __MySQL__ is not a convinient source, we can store the data to __HBase__.\n",
    "* Finally, when user accesses any of the __Ecommerce Servers__ they can see the generated __Recommendations__\n",
    "\n",
    "Above whole process can be scheduled (step-by-step running) using what we call __Oozie__. Also in this whole process __ZooKeeper__ is used internally by **HDFS**, **YARN** and **Hive**\n",
    "\n",
    "### What Data is Used for Recommendations:\n",
    "\n",
    "* The user data, i.e __UserId__, __ProductId__ and __Ratings(out of 5)__ in Tabular form is given to Spark Machine Learning library called __MLlib__.\n",
    "* This MLlib library return us another table having following three columns: __UserId__, __Recommended ProductId__ and __Suggested Ratings (Out of 5)__\n",
    "* Our __Logfile__ will look something like: __Logfile.txt: TimeStamp - IP Address - URL - UserId__ showing the records for (Time, IP Address, URL and UserId of a particular user)\n",
    "\n",
    "So, how will we convert such text Logfile into a tabular data frame?? \n",
    "In order to answer the above Question, we'll go through the following steps:\n",
    "\n",
    "1. Clean up the data where UserId isn't available\n",
    "2. Extract only the records where, a User visited a particular product (Not the general pages)\n",
    "3. Group the records based on __UserId__, __ProductId__ and prepare the counts (X user visited Y product, on Z timeStamp)\n",
    "4. Normalize the results, and bring them to out of 5. E.g a particular user has visite iPhoneX 20 times. So, it normalized the value of 20 out of 5 on average.\n",
    "\n",
    "Once this part is done. We need to read this data and write into the data frame, from here further we'll go ahead and write it into the __Hive__.\n",
    "\n",
    "Like we discussed above, if not convinient we directly store these recommendations into __HBase__ rather than in __SQL Webserver Database__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Queues:\n",
    "\n",
    "In case of Ecommerce website, Message Queues are messaging tools, that run asynchronously, i.e Vender can update details of the products and at the same time it's visible to users, without any wait.\n",
    "\n",
    "* Amazong -> SOS, Kafka\n",
    "* Spark Streaming\n",
    "\n",
    "Message comes into the message queue __Kafka__. From here, one copy goes to __HDFS__ and the other one goes to __Analytical Platform__. This phenomina also called __Lambda Architecture__. Kafka and Storm are also alternatives to Spark Streaming\n",
    "\n",
    "An interesting thing about __Spark__ is anything that __Java__ can do __Spark__ can do. Spark Streaming has latency of 1-5, so if we wish to built a super fast responsive servers, we are not going to use Spark Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
