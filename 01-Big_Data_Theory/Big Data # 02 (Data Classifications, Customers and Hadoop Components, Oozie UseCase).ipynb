{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Classification:\n",
    "\n",
    "Data is largely classified into three major types:\n",
    "\n",
    "* Structured Data: Field and Field's data types both are known. e.g RDBMSs such as (MySQL, Microsoft SQL and Oracle etc.)\n",
    "* Semi-Structured Data: Fields are known, but Field's Data Types are not known. e.g (Data in CSV files)\n",
    "* Unstructured Data: Neither Fields nor their data types are known. e.g (Plane text files & logs generated on Server)\n",
    "\n",
    "## ETL:\n",
    "\n",
    "The process of extracting __Unstructured Data__ apply some sort of transformation on it, and converting it to __Structured Data__ is called __ETL__.\n",
    "\n",
    "__ETL -> Extract, Transform & Load__\n",
    "\n",
    "## Distributed Systems:\n",
    "\n",
    "When Network computers are utilized to perform a common goal, it is known as __Distributed System__. The work gets distributed among many computers. The branch that studies distributed systems is known as __Distributed Computing__. The purpose of Distributed Computing is to get the work done faster, by utilizing many computers.\n",
    "Most but not all the tasks can be done using Distributed Computing.\n",
    "* Group of Network Computers\n",
    "* Interact with each other\n",
    "* To achieve a common goal.\n",
    "\n",
    "## What is Big Data:\n",
    "\n",
    "In simple words, __Big Data__ is very large amount of data, which can not be processed using usual tools. And to process such data, we need __Distributed Computing__. This data could be _Structured_ or _Unstructured_.\n",
    "\n",
    "Generally we classify the problems related to handling the Data into three buckets.\n",
    "\n",
    "* Volume (Data at rest) -> When the problem is related to storage of data. For example, __Facebook__ handling around __600TB__ of Data per day.\n",
    "* \n",
    "* Velocity (Data in motion) -> When we are trying to handle the data which is coming at faster rates. For example Connection requests send or received on facebook.\n",
    "* Variety -> Problems involving complex Data Structures, such as __Maps__, __Social Graphs__ and __Recommendations__ etc.\n",
    "\n",
    "Imagine, you have to find the __fastest route__ on a Map. It is complex to find the fastest and shorted path, eventhough the Map size wouldn't be too large.\n",
    "\n",
    "So as a bottom line, \"Data could be called as Big Data, if either __Volume__, __Velocity__ and or __Variety__ becomes impossible to handle using Traditional Tools\"\n",
    "\n",
    "## Big Data Customers:\n",
    "\n",
    "So far, we've tried to establish that while handling _Humongous Data_ we'd need new set of tools which can operate in a distributed fashion. But who would be generating such large amount of data?? A quick answer is __Everyone__. Let's take few examples:\n",
    "\n",
    "In Ecommerce industy the __Recommendation__ is a great example of __Big Data Processing__. Recommedations also known as __Collaborative Filtering__ is the process of suggesting someone a product based on their __preferences__. It basically tries to find __Similar Users__ and then cross suggest them the product. So, more the users better would be the results. Major chunk of __Amazon's sales__ happens via their recommendation systems. \n",
    "\n",
    "The engine __Spark MLlib__ has made it very simple to generate __Recommendations Systems__. All we have to do is Structurize the data in __UserId__, __ProductId__ and __Product Rating__ and feed it to the library such as MLlib.\n",
    "\n",
    "## The A/B Testing:\n",
    "\n",
    "Many a times large __Ecommerce__ platforms conducts a survey called A/B testing, in order to observe the users response and optimize their services as per user's ease. In A/B testing, half of the website's traffice is show version (A) of the webstie, and half of the traffice is show version (B/modified) of the same website. Then a statistical analysis is made based on user experience, which shows the user engagement with the basic and modified version. On the basis of this, website can improve it's interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Solutions:\n",
    "\n",
    "We have number of Big Data Processing engines that can provide the capability of processing large amount of data. Some of them are listed below..\n",
    "\n",
    "* Apache Hadoop ->  Provides storage of __Structured__ and __Unstructured__ data.\n",
    "* Apache Spark -> Provies Computational Capability on top of Hadoop.\n",
    "* Cassandra\n",
    "* MongoDB\n",
    "* Google Compute Engine or Microsoft Azure (have to upload the data to the service provider's server, which may not be acceptible sometimes by the organization)\n",
    "\n",
    "## What is Hadoop:\n",
    "\n",
    "Hadoop is under __Apache Liscence__, which means we can use it Open Sourced and without having to worry about the liscensing. Hadoop was created by __Doug Cutting__. It's based on GFS (Google file system), GMR (Google Map Reduce) and GBT (Google Big Table). The purpose of __Hadoop__ is to handle __Big Data__. Hadoop is distributed and also scalable (if needed, can add more machines to the existing system) and reliabe (if any of the machine fails, the system can still be running). It's is written in __Java__ that makes it also compatible with all kinds of devices.\n",
    "\n",
    "## Hadoop Components:\n",
    "Following are the components of Hadoop:\n",
    "\n",
    "* HDFS (Hadoop Distributed File System): The most important component, the entire echosystem depends on it. It is basically a file system that runs on multiple computers to provide large storage.\n",
    "* Yarn (Yet Another Resource Negotiator): Keeps track of all the resource, such as CPA and Memory of the machines in the network an run the applications. Any application which wants run from HDFS will communicate with Yarn.\n",
    "* HBase: Provide large amount of storage in the form of Data base tables. A kind of __NoSQL__ data storage\n",
    "* MapReduce: __computational framework__ for __Distributed Computing__. Utilizes __Yarn__ to execute programs. The __Map__ part transforms the raw data into __key__ value and the __Reduce__ part groups and combines data based on the __key__.\n",
    "* Spark: Another __computational framework__ similar to __MapReduce__ but faster and more recent.\n",
    "* Hive: It a __SQL interface__ which allows to write code in __SQL__ and converts it to __MapReduce__.\n",
    "* Pig (Latin): __SQL like language__. To express our __ETL__ needs in stack wise fashion. __Pig__ is the Engine that translates __Pig Latin__ into __MapReduce__, and execute it on __Hadoop__.\n",
    "* Mahout: A library of __Machine Learning__ algorithms, which runs in a distributed fashion. It reduces the complexity of ML algorithms running on many machines on MapReduce.\n",
    "* Flume: Helps pumping the data (struc. or unstruc.) to store it on centralized base such as HDFS\n",
    "* Sqoop: Used to transport data between __Hadoop__ and __SQL data base__. Sqoop utilized MapReduce to efficiently tranport data using many machines in the network.\n",
    "\n",
    "A typical __Big Data__ project might involve, importing data from __SQL Server__, running some __Hive__ querries, doing predictions with __Mahout__ and saving data back to __SQL Server__. This kind of workflow can be accomplish with __Oozie__. A user can talk to various components of Hadoop Echosystem using __Command Line__, __APIs__ or __Oozie__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oozie UseCase (generating recommendations):\n",
    "\n",
    "Oozie is like a __Workflow Engine__ that determines, which suggests the steps order.\n",
    "\n",
    "A typical Big Data recommendation system project, would be something like this:\n",
    "\n",
    "* First of all, from the __Web servers__ we will have to bring the data (Server logs) to HDFS. __Flume -> HDFS__\n",
    "* Data translation is done using __Pig, Hive, MapReduce and Spark__.\n",
    "* Save the results back to HDFS. (This is where the __ETL__ is happening step # 2 & 3)\n",
    "* Once we have this __Structured Data__ on our Hadoop server, we feed this data in to ML Library __Spark MLlib__ the performs machine learing and rate the recommendations. This MLlib saves the result back to HDFS.\n",
    "* Once the results was save in HDFS, the data is exported the our web server (MySQL) using __Sqoop__.\n",
    "* Finally from MySQL the web server will pull the recommendations and display them to the end user.\n",
    "\n",
    "As a end user, either you'll have to interact with Oozie, or use CLI/Python/Java to interact with the rest of Hadoop components. \n",
    "\n",
    "So, as we saw above in a typical recommendation system, all the tools of Hadoop Echosystem are used. Most importantly the steps needs to be in sequence. It deletes the previous data stored on HDFS after having newly modified data.\n",
    "\n",
    "This is how we generate the recommendations for user, based on Apache Server logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race Condition: \n",
    "When two process are trying to acces the same resources and the same time causing unpredictable results. For example: Two persons trying to deposit $1 to some bank account, and insted to amount to be credited by the factor __x+2__ it increments to __x+1__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadlock:\n",
    "\n",
    "When two process are waiting for each other, it is called __Deadlock__. For examples, processing running in a __Queue__ waiting for each other to be executed at first. In this can none of the processes will be executed.\n",
    "\n",
    "__Process1 -> Process2 -> Process3 -> Process1 -> Process2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
