{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Overview:\n",
    "\n",
    "In this section we're going to discuss a few __Big Data__ processing frameworks. For example we're going to go through the explanation of the following things:\n",
    "\n",
    "* Hadoop, MapReduce, Spart and PySpark\n",
    "* Local Vs Distributed systems\n",
    "* Overview of the Hadoop Echosystem\n",
    "* Detailed Overview of Spark\n",
    "* Set-up everything we need like (Jupyter, Spark) on __Amazon Web Services__\n",
    "* Resources on other Spark options\n",
    "* Jupyter Notebook hands-on code with __PySpark__ and __RDDs__\n",
    "\n",
    "We've worked with data that can fit on our local computer, in the scale of 0-8 GB. But what can we do if we have a larger set of Data.\n",
    "* Try using __SQL DATABASE__ to move storage into __Hard Drive__ insted of __RAM__\n",
    "* Or use a distrubited system, that distributes the data to multiple computers\n",
    "\n",
    "A local machine is our own computer, in which we are restricted to only __One Hard Drive__ and __One RAM__, but on a __Distributed Machine__ we can have __One Machine__ controlling a distribution of __Multiple Machines__\n",
    "\n",
    "* A local process will use the computation resources of a __Single Machine__\n",
    "* A distributed process has access to the computational resources across a No. of machines connected together.\n",
    "* Distributed machines also have the advantage of easily __Scaling__. We can simple add more machines.\n",
    "* Distributed machines also include __Fault Tolerance__, if one machine fails the whole process can still go.\n",
    "\n",
    "## Format of Distributed Architecture with Hadoop:\n",
    "\n",
    "* __Hadoop__ is a way to distribute very large files across multiple machines.\n",
    "* It uses __HDFS__ (Hadoop Distributed File System)\n",
    "* HDFS allows user to work with large datasets\n",
    "* HDFS also duplicates blocks of data for __Fault Tolerance__.\n",
    "* It also then uses __MapReduce__, which allows computations on __Large Scaled Data__.\n",
    "\n",
    "## MapReduce:\n",
    "\n",
    "* MapReduce is a way of __Splitting Computation__ task to a distributed set of files (such as HDFS).\n",
    "* It consists of Job Tracker and Multiple Task Trackers\n",
    "* Job Tracker sends code to run on the Task Tracker.\n",
    "* The Task Tracker allocates CPU and Memory for the tasks and monitor the tasks on the worker nodes.\n",
    "\n",
    "What we covered can be though of in two distinct parts:\n",
    "* Using __HDFS__ to distribute large datsets\n",
    "* Using __MapReduce__ to distribute a computational task to a distributed dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Overview:\n",
    "\n",
    "Let's go ahead and discuss the following things about __Spark__:\n",
    "* What is Spark\n",
    "* Spark Vs MapReduce\n",
    "* Spark RDDs\n",
    "* RDD Operations\n",
    "\n",
    "* Spark is one of the latest technologies being used to quickly and easily handle data.\n",
    "* It is an Open Source project on __Apache__\n",
    "* It was created at __AMPLab (UC Berkeley)__\n",
    "* We can think __Spark__ as a flexible alternative to __MapReduce__\n",
    "\n",
    "* Spark can use data stored in variety of formats e.g:\n",
    "    * Cassandra\n",
    "    * AWS S3\n",
    "    * HDFS\n",
    "    * And More..\n",
    "* __MapReduce__ requires files to be stored in __HDFS__, __Spark__ does not!\n",
    "* __Spark__ can also perform operations upto __100x__ faster than __MapReduce__.\n",
    "* __MapReduce__ writes most of it's data to disk after each map and reduce operation. While __Spark__ on the other hand keeps most of the data to the memory after each transformation. It can spill over to the disk of the memory is full.\n",
    "\n",
    "At the core of the __Spark__ is the idea of a __Resilient Distributed Dataset (RDD)__. It has four main features:\n",
    "* Distributed Collection of Data\n",
    "* Fault-tolerant\n",
    "* Parallel operation - partioned\n",
    "* Ability to use many data sources\n",
    "\n",
    "RDDs are immutable, lazily evaluated, and cacheable. There are two main types of RDD operations:\n",
    "* Transformations\n",
    "* Actions\n",
    "\n",
    "### Basic Actions:\n",
    "* First -> Return the first element in RDD\n",
    "* Collect -> Returns all the elements of RDD as an array at the driver program\n",
    "* Count -> Return the number of elements in RDD\n",
    "* Take -> Return an Array with the first 'n' elements, of the RDD\n",
    "\n",
    "### Basic Transformations:\n",
    "* Filter\n",
    "* Map\n",
    "* FlatMap\n",
    "\n",
    "### RDD.filter():\n",
    "* Applies a function to each element and returns elements that evaluate to __True__\n",
    "\n",
    "### RDD.map()\n",
    "* Transforms each element and preserves # of elements. Very similar idea to Pandas.apply() method\n",
    "\n",
    "### Map():\n",
    "* Grabbing first letter from list of names\n",
    "\n",
    "### FlatMap():\n",
    "* Transforming a corpus of text into a list of words!\n",
    "\n",
    "### Pair RDDs:\n",
    "Often RDDs will be holdig their values in tuples (key,value). This offers better partitioning of leads functionality based on reduction.\n",
    "\n",
    "### Reduce():\n",
    "* An action that will aggregate RDD elements using a function that returns a single element.\n",
    "\n",
    "### ReduceByKey():\n",
    "* An action that will aggregate Pair RDD elements using a function that returns a pair RDD. These ideas are similar to a Pandas __Groupby__ operations.\n",
    "\n",
    "Finally, the __Spark__ echosystem includes:\n",
    "* SparkSQL\n",
    "* Spark DataFrames\n",
    "* MLib\n",
    "* GraphX\n",
    "* Spark Streaming\n",
    "\n",
    "Now that we have learnt enough about __Apache Spark__, let's go ahead and set up the __Amazon Web Services__ account to get __Spark__ up and running!! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
